{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11909, 10724, 8809, 8312, 8844, 9784, 8423, 8823, 12333, 9503, 10156]\n",
      "paper_id  11909 title  The Wall and The Ball: A Study of Domain Referent Spreadsheet Errors category  HC\n",
      "paper_id  10724 title  CrashScope: A Practical Tool for Automated Testing of Android\n",
      "  Applications category  SE\n",
      "paper_id  8809 title  Examining the Impact of Platform Properties on Quality Attributes category  SE\n",
      "paper_id  8312 title  EuSpRIG 2006 Commercial Spreadsheet Review category  SE\n",
      "paper_id  8844 title  Ontology for Mobile Phone Operating Systems category  SE\n",
      "paper_id  9784 title  Service-Oriented Architecture in Industrial Automation Systems - The\n",
      "  case of IEC 61499: A Review category  SE\n",
      "paper_id  8423 title  Software Components for Web Services category  SE\n",
      "paper_id  8823 title  Towards a better understanding of testing if conditionals category  SE\n",
      "paper_id  12333 title  HTTP Mailbox - Asynchronous RESTful Communication category  SE\n",
      "paper_id  9503 title  Hierarchical Variability Modeling for Software Architectures category  SE\n",
      "paper_id  10156 title  Software Architecture Decision-Making Practices and Challenges: An\n",
      "  Industrial Case Study category  SE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "def tocsv_hybrid(uid):\n",
    "    import pickle\n",
    "    import csv\n",
    "    with open('student_paper_mappings.pkl', 'rb') as f:\n",
    "        data_ref = pickle.load(f)\n",
    "        matrix=data_ref.as_matrix()\n",
    "        for index,i in enumerate(matrix):\n",
    "            matrix[index][0]=matrix[index][0].split(',')\n",
    "\n",
    "        arr=[]\n",
    "        for (index,it1) in enumerate(matrix):\n",
    "            arr.append([])\n",
    "            for it2 in matrix:\n",
    "\n",
    "                it1_set = set(it1[0])\n",
    "                it2_set = set(it2[0])\n",
    "                arr[index].append(it1_set & it2_set)\n",
    "        # set (i,i ) to empty set before finding most matched user\n",
    "        for (index,it) in enumerate(arr):\n",
    "            arr[index][index]=set()\n",
    "        # Arrays to maintain the most matched user, and no. of common papers\n",
    "        match=[0]*120\n",
    "        count=[0]*120\n",
    "        # for i, val in enumerate(arr):\n",
    "        #     print(i, val)\n",
    "        for (index1,it2) in enumerate(arr):\n",
    "            #print(it2)\n",
    "            for(index2,it2) in enumerate(it2):\n",
    "                #print(index1,index2)\n",
    "                a = len(arr[index1][index2])\n",
    "                #print(len(arr[index1][index2]))\n",
    "                if(count[index1]<len(arr[index1][index2])):\n",
    "                    match[index1]=index2\n",
    "                    count[index1]=len(arr[index1][index2])\n",
    "        for (index,i) in enumerate(match):\n",
    "            s=(set(matrix[i][0])-set(matrix[index][0]))\n",
    "        l=list(s)\n",
    "        cleanedList = [x for x in l if str(x) != 'nan']\n",
    "        cl = [int(i) for i in cleanedList]\n",
    "\n",
    "        with open('papers.pkl', 'rb') as f:\n",
    "            data_papers = pickle.load(f)\n",
    "        uid=list(data_ref.student_id).index(uid)\n",
    "        s=(set(matrix[match[uid]][0])-set(matrix[uid][0]))\n",
    "        l=list(s)\n",
    "        cleanedList = [x for x in l if str(x) != 'nan']\n",
    "        cl = [int(i) for i in cleanedList]\n",
    "        print(cl)\n",
    "        list_papers=[]\n",
    "        with open('user_rec.csv','w') as csvfile:\n",
    "            fieldnames=['paper_id','title','category']\n",
    "            writer=csv.DictWriter(csvfile,fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for i in cl:\n",
    "                writer.writerow({'paper_id':i,'title':data_papers.title[i],'category':data_papers.arxiv_primary_category[i]})\n",
    "                list_papers.append(i)\n",
    "                #print(\"User ID \",uid)\n",
    "                print('paper_id ',i,'title ',data_papers.title[i],'category ',data_papers.arxiv_primary_category[i])\n",
    "            \n",
    "            \n",
    "tocsv_hybrid(621741617)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if __name__ == '__main__':\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There has been a noticeable shift in the relative composition of the industry\\nin the developed countries in recent years; manufacturing is decreasing while\\nthe service sector is becoming more important. However, currently most\\nsimulation models for investigating service systems are still built in the same\\nway as manufacturing simulation models, using a process-oriented world view,\\ni.e. they model the flow of passive entities through a system. These kinds of\\nmodels allow studying aspects of operational management but are not well suited\\nfor studying the dynamics that appear in service systems due to human\\nbehaviour. For these kinds of studies we require tools that allow modelling the\\nsystem and entities using an object-oriented world view, where intelligent\\nobjects serve as abstract \"actors\" that are goal directed and can behave\\nproactively. In our work we combine process-oriented discrete event simulation\\nmodelling and object-oriented agent based simulation modelling to investigate\\nthe impact of people management practices on retail productivity. In this\\npaper, we reveal in a series of experiments what impact considering proactivity\\ncan have on the output accuracy of simulation models of human centric systems.\\nThe model and data we use for this investigation are based on a case study in a\\nUK department store. We show that considering proactivity positively influences\\nthe validity of these kinds of models and therefore allows analysts to make\\nbetter recommendations regarding strategies to apply people management\\npractises.', 'The basic aim of our study is to give a possible model for handling uncertain\\ninformation. This model is worked out in the framework of DATALOG. At first the\\nconcept of fuzzy Datalog will be summarized, then its extensions for\\nintuitionistic- and interval-valued fuzzy logic is given and the concept of\\nbipolar fuzzy Datalog is introduced. Based on these ideas the concept of\\nmultivalued knowledge-base will be defined as a quadruple of any background\\nknowledge; a deduction mechanism; a connecting algorithm, and a function set of\\nthe program, which help us to determine the uncertainty levels of the results.\\nAt last a possible evaluation strategy is given.', 'Multi-step temporal-difference (TD) learning, where the update targets\\ncontain information from multiple time steps ahead, is one of the most popular\\nforms of TD learning for linear function approximation. The reason is that\\nmulti-step methods often yield substantially better performance than their\\nsingle-step counter-parts, due to a lower bias of the update targets. For\\nnon-linear function approximation, however, single-step methods appear to be\\nthe norm. Part of the reason could be that on many domains the popular\\nmulti-step methods TD($\\\\lambda$) and Sarsa($\\\\lambda$) do not perform well when\\ncombined with non-linear function approximation. In particular, they are very\\nsusceptible to divergence of value estimates. In this paper, we identify the\\nreason behind this. Furthermore, based on our analysis, we propose a new\\nmulti-step TD method for non-linear function approximation that addresses this\\nissue. We confirm the effectiveness of our method using two benchmark tasks\\nwith neural networks as function approximation.', 'Constraints that may be obtained by composition from simpler constraints are\\npresent, in some way or another, in almost every constraint program. The\\ndecomposition of such constraints is a standard technique for obtaining an\\nadequate propagation algorithm from a combination of propagators designed for\\nsimpler constraints. The decomposition approach is appealing in several ways.\\nFirstly because creating a specific propagator for every constraint is clearly\\ninfeasible since the number of constraints is infinite. Secondly, because\\ndesigning a propagation algorithm for complex constraints can be very\\nchallenging. Finally, reusing existing propagators allows to reduce the size of\\ncode to be developed and maintained. Traditionally, constraint solvers\\nautomatically decompose constraints into simpler ones using additional\\nauxiliary variables and propagators, or expect the users to perform such\\ndecomposition themselves, eventually leading to the same propagation model. In\\nthis paper we explore views, an alternative way to create efficient propagators\\nfor such constraints in a modular, simple and correct way, which avoids the\\nintroduction of auxiliary variables and propagators.', \"We mathematically model Ignacio Matte Blanco's principles of symmetric and\\nasymmetric being through use of an ultrametric topology. We use for this the\\nhighly regarded 1975 book of this Chilean psychiatrist and pyschoanalyst (born\\n1908, died 1995). Such an ultrametric model corresponds to hierarchical\\nclustering in the empirical data, e.g. text. We show how an ultrametric\\ntopology can be used as a mathematical model for the structure of the logic\\nthat reflects or expresses Matte Blanco's symmetric being, and hence of the\\nreasoning and thought processes involved in conscious reasoning or in reasoning\\nthat is lacking, perhaps entirely, in consciousness or awareness of itself. In\\na companion paper we study how symmetric (in the sense of Matte Blanco's)\\nreasoning can be demarcated in a context of symmetric and asymmetric reasoning\\nprovided by narrative text.\", 'We introduce the Binary Matrix Guessing Problem and provide two algorithms to\\nsolve this problem. The first algorithm we introduce is Elementwise Probing\\nAlgorithm (EPA) which is very fast under a score which utilizes Frobenius\\nDistance. The second algorithm is Additive Reinforcement Learning Algorithm\\nwhich combines ideas from perceptron algorithm and reinforcement learning\\nalgorithm. This algorithm is significantly slower compared to first one, but\\nless restrictive and generalizes better. We compare computational performance\\nof both algorithms and provide numerical results.\\n  reason for withdrawal: Paper will be rewritten with experiments replicated on\\nverified and validated hardware and software.', 'Concepts are the foundation of human deep learning, understanding, and\\nknowledge integration and transfer. We propose concept-oriented deep learning\\n(CODL) which extends (machine) deep learning with concept representations and\\nconceptual understanding capability. CODL addresses some of the major\\nlimitations of deep learning: interpretability, transferability, contextual\\nadaptation, and requirement for lots of labeled training data. We discuss the\\nmajor aspects of CODL including concept graph, concept representations, concept\\nexemplars, and concept representation learning systems supporting incremental\\nand continual learning.', 'Hybrid MKNF knowledge bases are one of the most prominent tightly integrated\\ncombinations of open-world ontology languages with closed-world (non-monotonic)\\nrule paradigms. The definition of Hybrid MKNF is parametric on the description\\nlogic (DL) underlying the ontology language, in the sense that non-monotonic\\nrules can extend any decidable DL language. Two related semantics have been\\ndefined for Hybrid MKNF: one that is based on the Stable Model Semantics for\\nlogic programs and one on the Well-Founded Semantics (WFS). Under WFS, the\\ndefinition of Hybrid MKNF relies on a bottom-up computation that has polynomial\\ndata complexity whenever the DL language is tractable. Here we define a general\\nquery-driven procedure for Hybrid MKNF that is sound with respect to the stable\\nmodel-based semantics, and sound and complete with respect to its WFS variant.\\nThis procedure is able to answer a slightly restricted form of conjunctive\\nqueries, and is based on tabled rule evaluation extended with an external\\noracle that captures reasoning within the ontology. Such an (abstract) oracle\\nreceives as input a query along with knowledge already derived, and replies\\nwith a (possibly empty) set of atoms, defined in the rules, whose truth would\\nsuffice to prove the initial query. With appropriate assumptions on the\\ncomplexity of the abstract oracle, the general procedure maintains the data\\ncomplexity of the WFS for Hybrid MKNF knowledge bases.\\n  To illustrate this approach, we provide a concrete oracle for EL+, a fragment\\nof the light-weight DL EL++. Such an oracle has practical use, as EL++ is the\\nlanguage underlying OWL 2 EL, which is part of the W3C recommendations for the\\nSemantic Web, and is tractable for reasoning tasks such as subsumption. We show\\nthat query-driven Hybrid MKNF preserves polynomial data complexity when using\\nthe EL+ oracle and WFS.', 'We describe a new paradigm for implementing inference in belief networks,\\nwhich consists of two steps: (1) compiling a belief network into an arithmetic\\nexpression called a Query DAG (Q-DAG); and (2) answering queries using a simple\\nevaluation algorithm. Each node of a Q-DAG represents a numeric operation, a\\nnumber, or a symbol for evidence. Each leaf node of a Q-DAG represents the\\nanswer to a network query, that is, the probability of some event of interest.\\nIt appears that Q-DAGs can be generated using any of the standard algorithms\\nfor exact inference in belief networks (we show how they can be generated using\\nclustering and conditioning algorithms). The time and space complexity of a\\nQ-DAG generation algorithm is no worse than the time complexity of the\\ninference algorithm on which it is based. The complexity of a Q-DAG evaluation\\nalgorithm is linear in the size of the Q-DAG, and such inference amounts to a\\nstandard evaluation of the arithmetic expression it represents. The intended\\nvalue of Q-DAGs is in reducing the software and hardware resources required to\\nutilize belief networks in on-line, real-world applications. The proposed\\nframework also facilitates the development of on-line inference on different\\nsoftware and hardware platforms due to the simplicity of the Q-DAG evaluation\\nalgorithm. Interestingly enough, Q-DAGs were found to serve other purposes:\\nsimple techniques for reducing Q-DAGs tend to subsume relatively complex\\noptimization techniques for belief-network inference, such as network-pruning\\nand computation-caching.', 'FLUX is a programming method for the design of agents that reason logically\\nabout their actions and sensor information in the presence of incomplete\\nknowledge. The core of FLUX is a system of Constraint Handling Rules, which\\nenables agents to maintain an internal model of their environment by which they\\ncontrol their own behavior. The general action representation formalism of the\\nfluent calculus provides the formal semantics for the constraint solver. FLUX\\nexhibits excellent computational behavior due to both a carefully restricted\\nexpressiveness and the inference paradigm of progression.', 'This paper has been withdrawn by the author due to extremely unscientific\\nerrors.', 'In this paper we present a short history of logics: from particular cases of\\n2-symbol or numerical valued logic to the general case of n-symbol or numerical\\nvalued logic. We show generalizations of 2-valued Boolean logic to fuzzy logic,\\nalso from the Kleene and Lukasiewicz 3-symbol valued logics or Belnap 4-symbol\\nvalued logic to the most general n-symbol or numerical valued refined\\nneutrosophic logic. Two classes of neutrosophic norm (n-norm) and neutrosophic\\nconorm (n-conorm) are defined. Examples of applications of neutrosophic logic\\nto physics are listed in the last section. Similar generalizations can be done\\nfor n-Valued Refined Neutrosophic Set, and respectively n- Valued Refined\\nNeutrosopjhic Probability.', 'Answer-set programming (ASP) has emerged recently as a viable programming\\nparadigm. We describe here an ASP system, DATALOG with constraints or DC, based\\non non-monotonic logic. Informally, DC theories consist of propositional\\nclauses (constraints) and of Horn rules. The semantics is a simple and natural\\nextension of the semantics of the propositional logic. However, thanks to the\\npresence of Horn rules in the system, modeling of transitive closure becomes\\nstraightforward. We describe the syntax, use and implementation of DC and\\nprovide experimental results.', \"We explore methods for option discovery based on variational inference and\\nmake two algorithmic contributions. First: we highlight a tight connection\\nbetween variational option discovery methods and variational autoencoders, and\\nintroduce Variational Autoencoding Learning of Options by Reinforcement\\n(VALOR), a new method derived from the connection. In VALOR, the policy encodes\\ncontexts from a noise distribution into trajectories, and the decoder recovers\\nthe contexts from the complete trajectories. Second: we propose a curriculum\\nlearning approach where the number of contexts seen by the agent increases\\nwhenever the agent's performance is strong enough (as measured by the decoder)\\non the current set of contexts. We show that this simple trick stabilizes\\ntraining for VALOR and prior variational option discovery methods, allowing a\\nsingle agent to learn many more modes of behavior than it could with a fixed\\ncontext distribution. Finally, we investigate other topics related to\\nvariational option discovery, including fundamental limitations of the general\\napproach and the applicability of learned options to downstream tasks.\", \"We introduce a new artificial intelligence (AI) approach called, the 'Digital\\nSynaptic Neural Substrate' (DSNS). It uses selected attributes from objects in\\nvarious domains (e.g. chess problems, classical music, renowned artworks) and\\nrecombines them in such a way as to generate new attributes that can then, in\\nprinciple, be used to create novel objects of creative value to humans relating\\nto any one of the source domains. This allows some of the burden of creative\\ncontent generation to be passed from humans to machines. The approach was\\ntested in the domain of chess problem composition. We used it to automatically\\ncompose numerous sets of chess problems based on attributes extracted and\\nrecombined from chess problems and tournament games by humans, renowned\\npaintings, computer-evolved abstract art, photographs of people, and classical\\nmusic tracks. The quality of these generated chess problems was then assessed\\nautomatically using an existing and experimentally-validated computational\\nchess aesthetics model. They were also assessed by human experts in the domain.\\nThe results suggest that attributes collected and recombined from chess and\\nother domains using the DSNS approach can indeed be used to automatically\\ngenerate chess problems of reasonably high aesthetic quality. In particular, a\\nlow quality chess source (i.e. tournament game sequences between weak players)\\nused in combination with actual photographs of people was able to produce\\nthree-move chess problems of comparable quality or better to those generated\\nusing a high quality chess source (i.e. published compositions by human\\nexperts), and more efficiently as well. Why information from a foreign domain\\ncan be integrated and functional in this way remains an open question for now.\\nThe DSNS approach is, in principle, scalable and applicable to any domain in\\nwhich objects have attributes that can be represented using real numbers.\", 'Search is a major technique for planning. It amounts to exploring a state\\nspace of planning domains typically modeled as a directed graph. However,\\nprohibitively large sizes of the search space make search expensive. Developing\\nbetter heuristic functions has been the main technique for improving search\\nefficiency. Nevertheless, recent studies have shown that improving heuristics\\nalone has certain fundamental limits on improving search efficiency. Recently,\\na new direction of research called partial order based reduction (POR) has been\\nproposed as an alternative to improving heuristics. POR has shown promise in\\nspeeding up searches.\\n  POR has been extensively studied in model checking research and is a key\\nenabling technique for scalability of model checking systems. Although the POR\\ntheory has been extensively studied in model checking, it has never been\\ndeveloped systematically for planning before. In addition, the conditions for\\nPOR in the model checking theory are abstract and not directly applicable in\\nplanning. Previous works on POR algorithms for planning did not establish the\\nconnection between these algorithms and existing theory in model checking.\\n  In this paper, we develop a theory for POR in planning. The new theory we\\ndevelop connects the stubborn set theory in model checking and POR methods in\\nplanning. We show that previous POR algorithms in planning can be explained by\\nthe new theory. Based on the new theory, we propose a new, stronger POR\\nalgorithm. Experimental results on various planning domains show further search\\ncost reduction using the new algorithm.', 'This paper shows that causal model discovery is not an NP-hard problem, in\\nthe sense that for sparse graphs bounded by node degree k the sound and\\ncomplete causal model can be obtained in worst case order N^{2(k+2)}\\nindependence tests, even when latent variables and selection bias may be\\npresent. We present a modification of the well-known FCI algorithm that\\nimplements the method for an independence oracle, and suggest improvements for\\nsample/real-world data versions. It does not contradict any known hardness\\nresults, and does not solve an NP-hard problem: it just proves that sparse\\ncausal discovery is perhaps more complicated, but not as hard as learning\\nminimal Bayesian networks.', 'Ontologies and rules are usually loosely coupled in knowledge representation\\nformalisms. In fact, ontologies use open-world reasoning while the leading\\nsemantics for rules use non-monotonic, closed-world reasoning. One exception is\\nthe tightly-coupled framework of Minimal Knowledge and Negation as Failure\\n(MKNF), which allows statements about individuals to be jointly derived via\\nentailment from an ontology and inferences from rules. Nonetheless, the\\npractical usefulness of MKNF has not always been clear, although recent work\\nhas formalized a general resolution-based method for querying MKNF when rules\\nare taken to have the well-founded semantics, and the ontology is modeled by a\\ngeneral oracle. That work leaves open what algorithms should be used to relate\\nthe entailments of the ontology and the inferences of rules. In this paper we\\nprovide such algorithms, and describe the implementation of a query-driven\\nsystem, CDF-Rules, for hybrid knowledge bases combining both (non-monotonic)\\nrules under the well-founded semantics and a (monotonic) ontology, represented\\nby a CDF Type-1 (ALQ) theory. To appear in Theory and Practice of Logic\\nProgramming (TPLP)', 'Humans learn complex latent structures from their environments (e.g., natural\\nlanguage, mathematics, music, social hierarchies). In cognitive science and\\ncognitive neuroscience, models that infer higher-order structures from sensory\\nor first-order representations have been proposed to account for the complexity\\nand flexibility of human behavior. But how do the structures that these models\\ninvoke arise in neural systems in the first place? To answer this question, we\\nexplain how a system can learn latent representational structures (i.e.,\\npredicates) from experience with wholly unstructured data. During the process\\nof predicate learning, an artificial neural network exploits the naturally\\noccurring dynamic properties of distributed computing across neuronal\\nassemblies in order to learn predicates, but also to combine them\\ncompositionally, two computational aspects which appear to be necessary for\\nhuman behavior as per formal theories in multiple domains. We describe how\\npredicates can be combined generatively using neural oscillations to achieve\\nhuman-like extrapolation and compositionality in an artificial neural network.\\nThe ability to learn predicates from experience, to represent structures\\ncompositionally, and to extrapolate to unseen data offers an inroads to\\nunderstanding and modeling the most complex human behaviors.', 'Obtaining more accurate equity value estimates is the starting point for\\nstock selection, value-based indexing in a noisy market, and beating benchmark\\nindices through tactical style rotation. Unfortunately, discounted cash flow,\\nmethod of comparables, and fundamental analysis typically yield discrepant\\nvaluation estimates. Moreover, the valuation estimates typically disagree with\\nmarket price. Can one form a superior valuation estimate by averaging over the\\nindividual estimates, including market price? This article suggests a Bayesian\\nframework for combining two or more estimates into a superior valuation\\nestimate. The framework justifies the common practice of averaging over several\\nestimates to arrive at a final point estimate.']\n",
      "['Despite strong evidence of widespread errors, spreadsheet developers rarely\\nsubject their spreadsheets to post-development testing to reduce errors. This\\nmay be because spreadsheet developers are overconfident in the accuracy of\\ntheir spreadsheets. This conjecture is plausible because overconfidence is\\npresent in a wide variety of human cognitive domains, even among experts. This\\npaper describes two experiments in overconfidence in spreadsheet development.\\nThe first is a pilot study to determine the existence of overconfidence. The\\nsecond tests a manipulation to reduce overconfidence and errors. The\\nmanipulation is modestly successful, indicating that overconfidence reduction\\nis a promising avenue to pursue.', 'Well-designed and publicly available datasets of bugs are an invaluable asset\\nto advance research fields such as fault localization and program repair as\\nthey allow directly and fairly comparison between competing techniques and also\\nthe replication of experiments. These datasets need to be deeply understood by\\nresearchers: the answer for questions like \"which bugs can my technique\\nhandle?\" and \"for which bugs is my technique effective?\" depends on the\\ncomprehension of properties related to bugs and their patches. However, such\\nproperties are usually not included in the datasets, and there is still no\\nwidely adopted methodology for characterizing bugs and patches. In this work,\\nwe deeply study 395 patches of the Defects4J dataset. Quantitative properties\\n(patch size and spreading) were automatically extracted, whereas qualitative\\nones (repair actions and patterns) were manually extracted using a thematic\\nanalysis-based approach. We found that 1) the median size of Defects4J patches\\nis four lines, and almost 30% of the patches contain only addition of lines; 2)\\n92% of the patches change only one file, and 38% has no spreading at all; 3)\\nthe top-3 most applied repair actions are addition of method calls,\\nconditionals, and assignments, occurring in 77% of the patches; and 4) nine\\nrepair patterns were found for 95% of the patches, where the most prevalent,\\nappearing in 43% of the patches, is on conditional blocks. These results are\\nuseful for researchers to perform advanced analysis on their techniques\\'\\nresults based on Defects4J. Moreover, our set of properties can be used to\\ncharacterize and compare different bug datasets.', 'The wide variety of wireless devices brings to design mobile applications as\\na collection of interchangeable software components adapted to the deployment\\nenvironment of the software. To ensure the proper functioning of the software\\nassembly and make a real enforcement in case of failures, the introduction of\\nconcepts, models and tools necessary for the administration of these components\\nis crucial. This article proposes a method for deploying components in wireless\\nsystems.', 'Our term \"structure discovery\" denotes the recovery of structure, such as the\\ngrouping of cells, that was intended by a spreadsheet\\'s author but is not\\nexplicit in the spreadsheet. We are implementing structure discovery tools in\\nthe logic-programming language Prolog for our spreadsheet analysis program\\nModel Master, by writing grammars for spreadsheet structures. The objective is\\nan \"intelligent structure monitor\" to run beside Excel, allowing users to\\nreconfigure spreadsheets to the representational needs of the task at hand.\\nThis could revolutionise spreadsheet \"best practice\". We also describe a\\nformulation of spreadsheet reverse-engineering based on \"arrows\".', 'Feed syndication is analogous to electronic newsletters, both are aimed at\\ndelivering feeds to subscribers; the difference is that while newsletter\\nsubscription requires e-mail and exposed you to spam and security challenges,\\nfeed syndication ensures that you only get what you requested for. This paper\\nreports a review on the state of the art of feed aggregation technology and the\\ndevelopment of a locally hosted web based feed aggregator as a research tool\\nusing the core features of WordPress; the software was further enhanced with\\nplugins and widgets for dynamic content publishing, database and object\\ncaching, social web syndication, back-up and maintenance, among others. The\\nresults highlight the current developments in software re-use and describes;\\nhow open source content management systems can be used for both online and\\noffline publishing, a means whereby feed aggregator users can control and share\\nfeed data, as well as how web developers can focus on extending the features of\\nbuilt-in software libraries in applications rather than reinventing the wheel.', 'A common and natural intuition among software testers is that test cases need\\nto differ if a software system is to be tested properly and its quality\\nensured. Consequently, much research has gone into formulating distance\\nmeasures for how test cases, their inputs and/or their outputs differ. However,\\ncommon to these proposals is that they are data type specific and/or calculate\\nthe diversity only between pairs of test inputs, traces or outputs.\\n  We propose a new metric to measure the diversity of sets of tests: the test\\nset diameter (TSDm). It extends our earlier, pairwise test diversity metrics\\nbased on recent advances in information theory regarding the calculation of the\\nnormalized compression distance (NCD) for multisets. An advantage is that TSDm\\ncan be applied regardless of data type and on any test-related information, not\\nonly the test inputs. A downside is the increased computational time compared\\nto competing approaches.\\n  Our experiments on four different systems show that the test set diameter can\\nhelp select test sets with higher structural and fault coverage than random\\nselection even when only applied to test inputs. This can enable early test\\ndesign and selection, prior to even having a software system to test, and\\ncomplement other types of test automation and analysis. We argue that this\\nquantification of test set diversity creates a number of opportunities to\\nbetter understand software quality and provides practical ways to increase it.', 'Defect Prevention is the most critical but most neglected component of the\\nsoftware quality assurance in any project. If applied at all stages of software\\ndevelopment, it can reduce the time, cost and resources required to engineer a\\nhigh quality product. Software inspection has proved to be the most effective\\nand efficient technique enabling defect detection and prevention. Inspections\\ncarried at all phases of software life cycle have proved to be most beneficial\\nand value added to the attributes of the software. Work is an analysis based on\\nthe data collected for three different projects from a leading product based\\ncompany. The purpose of the paper is to show that 55% to 65% of total number of\\ndefects occurs at design phase. Position of this paper also emphasizes the\\nimportance of inspections at all phases of the product development life cycle\\nin order to achieve the minimal post deployment defects.', \"The nature and complexity of software have changed significantly in the last\\nfew decades. With the easy availability of computing power, deeper and broader\\napplications are made. It has been extremely necessary to produce good quality\\nsoftware with high precession of reliability right in the first place. Olden\\nday's software errors and bugs were fixed at a later stage in the software\\ndevelopment. Today to produce high quality reliable software and to keep a\\nspecific time schedule is a big challenge. To cope up the challenge many\\nconcepts, methodology and practices of software engineering have been evolved\\nfor developing reliable software. Better methods of controlling the process of\\nsoftware production are underway. One of such methods to assess the software\\nreliability is using control charts. In this paper we proposed an NHPP based\\ncontrol mechanism by using order statistics with cumulative quantity between\\nobservations of failure data using mean value function of exponential\\ndistribution.\", 'Orchestrating centralised service-oriented workflows presents significant\\nscalability challenges that include: the consumption of network bandwidth,\\ndegradation of performance, and single points of failure. This paper presents a\\nhigh-level dataflow specification language that attempts to address these\\nscalability challenges. This language provides simple abstractions for\\norchestrating large-scale web service workflows, and separates between the\\nworkflow logic and its execution. It is based on a data-driven model that\\npermits parallelism to improve the workflow performance. We provide a\\ndecentralised architecture that allows the computation logic to be moved\\n\"closer\" to services involved in the workflow. This is achieved through\\npartitioning the workflow specification into smaller fragments that may be sent\\nto remote orchestration services for execution. The orchestration services rely\\non proxies that exploit connectivity to services in the workflow. These proxies\\nperform service invocations and compositions on behalf of the orchestration\\nservices, and carry out data collection, retrieval, and mediation tasks. The\\nevaluation of our architecture implementation concludes that our decentralised\\napproach reduces the execution time of workflows, and scales accordingly with\\nthe increasing size of data sets.', \"Modeling of software architectures is a fundamental part of software\\ndevelopment processes. Reuse of software components and early analysis of\\nsoftware topologies allow the reduction of development costs and increases\\nsoftware quality. Integrating variability modeling concepts into architecture\\ndescription languages (ADLs) is essential for the development of diverse\\nsoftware systems with high demands on software quality. In this paper, we\\npresent the integration of delta modeling into the existing ADL MontiArc. Delta\\nmodeling is a language-independent variability modeling approach supporting\\nproactive, reactive and extractive product line development. We show how\\n?-MontiArc, a language for explicit modeling of architectural variability based\\non delta modeling, is implemented as domain-specific language (DSL) using the\\nDSL development framework MontiCore. We also demonstrate how MontiCore's\\nlanguage reuse mechanisms provide efficient means to derive an implementation\\nof ?-MontiArc tool implementation. We evaluate ?-Monti-Arc by comparing it with\\nannotative variability modeling.\", \"Model composition plays a central role in many software engineering\\nactivities such as evolving models to add new features and reconciling\\nconflicting design models developed in parallel by different development teams.\\nAs model composition is usually an error-prone and effort-consuming task, its\\npotential benefits, such as gains in productivity can be compromised. However,\\nthere is no empirical knowledge nowadays about the effort required to compose\\ndesign models. Only feedbacks of model composition evangelists are available,\\nand they often diverge. Consequently, developers are unable to conduct any\\ncost-effectiveness analysis as well as identify, predict, or reduce composition\\neffort. The inability of evaluating composition effort is due to three key\\nproblems. First, the current evaluation frameworks do not consider fundamental\\nconcepts in model composition such as conflicts and inconsistencies. Second,\\nresearchers and developers do not know what factors can influence the\\ncomposition effort in practice. Third, practical knowledge about how such\\ninfluential factors may affect the developers' effort is severely lacking. In\\nthis context, the contributions of this thesis are threefold: (i) a quality\\nmodel for supporting the evaluation of model composition effort, (ii) practical\\nknowledge, derived from a family of quantitative and qualitative empirical\\nstudies, about model composition effort and its influential factors, and (iii)\\ninsight about how to evaluate model composition efforts and tame the side\\neffects of such influential factors.\"]\n",
      "                                             summary\n",
      "0  Despite strong evidence of widespread errors, ...\n",
      "1  Well-designed and publicly available datasets ...\n",
      "2  The wide variety of wireless devices brings to...\n",
      "3  Our term \"structure discovery\" denotes the rec...\n",
      "4  Feed syndication is analogous to electronic ne...\n",
      "                                        user_summary\n",
      "0  There has been a noticeable shift in the relat...\n",
      "1  The basic aim of our study is to give a possib...\n",
      "2  Multi-step temporal-difference (TD) learning, ...\n",
      "3  Constraints that may be obtained by compositio...\n",
      "4  We mathematically model Ignacio Matte Blanco's...\n",
      "0     Despite strong evidence of widespread errors, ...\n",
      "1     Well-designed and publicly available datasets ...\n",
      "2     The wide variety of wireless devices brings to...\n",
      "3     Our term \"structure discovery\" denotes the rec...\n",
      "4     Feed syndication is analogous to electronic ne...\n",
      "5     A common and natural intuition among software ...\n",
      "6     Defect Prevention is the most critical but mos...\n",
      "7     The nature and complexity of software have cha...\n",
      "8     Orchestrating centralised service-oriented wor...\n",
      "9     Modeling of software architectures is a fundam...\n",
      "10    Model composition plays a central role in many...\n",
      "Name: summary, dtype: object\n",
      "['despit', 'strong', 'evid', 'widespread', 'error', 'spreadsheet', 'develop', 'rare', 'subject', 'spreadsheet', 'postdevelop', 'test', 'reduc', 'error', 'may', 'spreadsheet', 'develop', 'overconfid', 'accuraci', 'spreadsheet', 'conjectur', 'plausibl', 'overconfid', 'present', 'wide', 'varieti', 'human', 'cognit', 'domain', 'even', 'among', 'expert', 'paper', 'describ', 'two', 'experi', 'overconfid', 'spreadsheet', 'develop', 'first', 'pilot', 'studi', 'determin', 'exist', 'overconfid', 'second', 'test', 'manipul', 'reduc', 'overconfid', 'error', 'manipul', 'modestli', 'success', 'indic', 'overconfid', 'reduct', 'promis', 'avenu', 'pursu']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['basic', 'aim', 'studi', 'give', 'possibl', 'model', 'handl', 'uncertain', 'inform', 'model', 'work', 'framework', 'datalog', 'first', 'concept', 'fuzzi', 'datalog', 'summar', 'extens', 'intuitionist', 'intervalvalu', 'fuzzi', 'logic', 'given', 'concept', 'bipolar', 'fuzzi', 'datalog', 'introduc', 'base', 'idea', 'concept', 'multivalu', 'knowledgebas', 'defin', 'quadrupl', 'background', 'knowledg', 'deduct', 'mechan', 'connect', 'algorithm', 'function', 'set', 'program', 'help', 'us', 'determin', 'uncertainti', 'level', 'result', 'last', 'possibl', 'evalu', 'strategi', 'given']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:64: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:65: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08202216692278198,\n",
       " 0.06532181680580171,\n",
       " 0.06187622439394684,\n",
       " 0.058321184351980436,\n",
       " 0.057827625257403106,\n",
       " 0.04067819981796425,\n",
       " 0.040177170184515394,\n",
       " 0.03575421226192905,\n",
       " 0.019676044536698512,\n",
       " 0.0]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def content(papers,uid):\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "\n",
    "    with open('papers.pkl', 'rb') as f :\n",
    "        data = pickle.load(f)\n",
    "    with open('student_paper_mappings.pkl', 'rb') as f:\n",
    "        data_ref = pickle.load(f)\n",
    "        matrix=data_ref.as_matrix()\n",
    "        for index,i in enumerate(matrix):\n",
    "            matrix[index][0]=matrix[index][0].split(',')\n",
    "    uid=list(data_ref.student_id).index(uid)\n",
    "    mat=data.as_matrix()\n",
    "    user_summar=[]\n",
    "    for i in matrix[uid][0]:\n",
    "        user_summar.append(mat[int(i)-1][5])\n",
    "       \n",
    "        \n",
    "    \n",
    "    summar=[]\n",
    "    for i in papers:\n",
    "        summar.append(mat[int(i)-1][5])\n",
    "    print(user_summar) \n",
    "    print(summar)\n",
    "    summar_new = pd.DataFrame({'summary':summar})\n",
    "    print(summar_new.head())\n",
    "    summary=summar_new[\"summary\"]\n",
    "    \n",
    "    user_summar_new = pd.DataFrame({'user_summary':user_summar})\n",
    "    print(user_summar_new.head())\n",
    "    user_summary=user_summar_new[\"user_summary\"]\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    set(stopwords.words('english'))\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    print(summary)\n",
    "    def tokenize_summary(summary):\n",
    "        summ = [char for char in summary if char not in string.punctuation]\n",
    "        summ = ''.join(summ)\n",
    "    \n",
    "        word = [word for word in summ.split() if word.lower() not in stopwords.words('english')]\n",
    "        return[porter_stemmer.stem(word) for word in word]\n",
    "    print(tokenize_summary(summary[0]))\n",
    "    \n",
    "    summary_tokens = []\n",
    "    for i in summary:\n",
    "            summary_tokens.append(tokenize_summary(i))\n",
    "    \n",
    "    user_summary_tokens = []\n",
    "    for i in user_summary:\n",
    "            user_summary_tokens.append(tokenize_summary(i))\n",
    "            \n",
    "            \n",
    "    print(user_summary_tokens[1])\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc,lowercase=False,stop_words='english', use_idf=False, norm='l1')\n",
    "    Y = vectorizer.fit_transform(summary_tokens)\n",
    "    X = vectorizer.transform(user_summary_tokens)\n",
    "    X=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    Y= pd.DataFrame(Y.toarray(), columns=vectorizer.get_feature_names())\n",
    "    X=X.as_matrix()\n",
    "    Y=Y.as_matrix()\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    for index,i in enumerate(X):\n",
    "        similarity = (cosine_similarity(X[index:index+1], Y))\n",
    "        flattened_list = [y for x in similarity for y in x]\n",
    "        similarity_sorted=sorted(flattened_list,reverse=True)\n",
    "        similarity_topn = similarity_sorted[1:11]\n",
    "    return similarity_topn\n",
    "\n",
    "content([11909, 10724,  8809,  8312,  8844,  9784,  8423,  8823, 12333,  9503,\n",
    " 10156],621741617)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
